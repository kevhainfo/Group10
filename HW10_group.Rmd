---
title: "HW10 final project"
author:
- Vayun Malik
- Karam Malhotra
- Kedaar Rentachintala
- Kenneth Ren
- Kevin Ha
date: 'Due: Second Friday of Instruction'
output:
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---

```{r}
library(readr)
library(ggplot2)
test_data <- read_csv("~/Wharton DSA/Day 10/balanced_dataset.csv")
```
This homework serves as the second milestone towards the final project. You can submit a draft of your project that includes the following components.

**1. Goal of study**. Restate the goal and importance of your project. Did you change your goal of study due to availability of data/consulting TF/etc?

The goal of our data is to study trends in email scams. Our primary focus of scams is email scams. One of our datasets contains a comprehensive list describing email job advertisement scams. We plan to eventually create a neural network that can analyze this complex data to predict the likelihood of a certain email being a scam.

Originally, we wanted to focus more on the impacts of scams on disabled people. However, we were unable to find any reliable data about this topic. As a result, we decided to focus on scams in general.

**2. Data:**

As we mentioned and as you may have already experienced, we, data scientists, usually spend 70% or more time on data acquisition, wrangling to make data in shape. At the same time, to make your results reproducible, it is important to document the entire process. Therefore, in your report, you should always reserve a section to talk about the data.

i) What is your data source?
https://www.kaggle.com/datasets/amruthjithrajvr/recruitment-scam
ii) Describe how you acquire the data. (You can provide another set of code).

We found data that pertained to our research question on Kaggle. After this we realized that the data was unbalanced and only had a few values that were actually scams. To account for this we balanced the dataset using Jupyter Notebook. If you are curious the code that we used to do this is in the file called extractingBalancedDataset.ipynb in our folder.

iii) Describe how you clean the data (e.g. missing value, errors, outliners).

Some columns in our email job scam dataset were categorical. When these categorical variables were missing, we were able to simply replace the `NA` values with existing categories such as “Unspecified”. This was done with the code block shown below. Additionally, one of the variables depicted the salary range so we split it into two quantitative variables which were the minimum salary and maximum salary. This was done with the salary code block in the EDA section.

```{r Cleaning 1}
test_data$required_experience[is.na(test_data$required_experience)] <- "Not Applicable"
test_data$required_education[is.na(test_data$required_education)] <- "Unspecified"
test_data$employment_type[is.na(test_data$employment_type)] <- "Unspecified"
```


iv) Is your data representative enough for your goal of study? Could there be any bias? How will you address the bias if any?

We are not fully sure. The Kaggle page for the recruitment scam dataset indicates that the data is from Greece, but the website that contains the source of the website (http://emscad.samos.aegean.gr/) is unavailable, so we cannot measure the bias. Although the data has been mostly converted into a general format that can apply to any location, there still may be bias due to difference in scams between different countries.

**3. EDA**

As the first step of our analysis, we need to make sense out of the data by EDA. 
Please provide a complete EDA of your data using relevant plots (with title, labels, colors, legends) and descriptions. You should always bear in mind your goal of study.

Two of the variables in our dataset that we didn't end up using for our analysis were title and department as they were categorical and just described some basic information about each scam.

```{r salary}
min_sal = c()
max_sal = c()
for (index in 1:nrow(test_data)) {
  temp <-  strsplit(test_data$salary_range[index], split = "-")
  min_sal <- append(min_sal, temp[[1]][1])
  max_sal <- append(max_sal, temp[[1]][2])
}
test_data$min_sal = as.numeric(min_sal)
test_data$max_sal = as.numeric(max_sal)
hist(test_data$min_sal, col = "blue",  main = "Minimum Salary", xlab = "Salary", breaks = 30)
hist(test_data$max_sal, col = "blue",  main = "Maximum Salary", xlab = "Salary", breaks = 30)
mean(test_data$min_sal, na.rm = TRUE)
mean(test_data$max_sal, na.rm = TRUE)
fivenum(test_data$min_sal, na.rm = TRUE)
fivenum(test_data$max_sal, na.rm = TRUE)
```

```{r employment_type & required_education & required_experience}
table(test_data$employment_type, useNA = "ifany")
table(test_data$required_education, useNA = "ifany")
table(test_data$required_experience, useNA = "ifany")
```

```{r Employment Type}
ggplot(test_data, aes(test_data$employment_type, fill=test_data$fraudulent)) + 
    geom_bar(position="stack") +
    ggtitle("Number of types of employment") +
    xlab("Type of employment") +
    ylab("Count") +
    scale_fill_manual(values=c('sky blue', 'dark red'))
```

```{r Required Experience}
ggplot(test_data, aes(test_data$required_experience, fill=test_data$fraudulent)) + 
    geom_bar(position="stack") +
    ggtitle("Number of types of experience") +
    xlab("Type of experience") +
    ylab("Count") +
    scale_fill_manual(values=c('sky blue', 'dark red')) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

```

```{r Required Education}
ggplot(test_data, aes(test_data$required_education, fill=test_data$fraudulent)) + 
    geom_bar(position="stack") +
    ggtitle("Number of types of employment") +
    xlab("Type of education") +
    ylab("Count") +
    scale_fill_manual(values=c('sky blue', 'dark red')) + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

```{r Telecommuting, echo=FALSE}

# Create a bar plot for Telecommuting
ggplot(test_data, aes(x = telecommuting)) +
  geom_bar(fill = "blue") +
  labs(x = "Telecommuting", y = "Frequency") +
  ggtitle("Frequency of Telecommuting")

# Pie chart
  values <- c(sum(test_data$telecommuting == FALSE), sum(test_data$telecommuting == TRUE)) 
  labels <- c("Has Telecommuting", "Doesn't Have Telecommuting")
  pie(values, labels = labels, radius = 0.6, donut = TRUE)
```

```{r Has_company_logo}
# Create a bar plot for Has_company_logo
ggplot(test_data, aes(x = has_company_logo)) +
  geom_bar(fill = "green") +
  labs(x = "Has_company_logo", y = "Frequency") +
  ggtitle("Frequency of Having Company Logo")

# Pie chart
  values <- c(sum(test_data$has_company_logo == FALSE), sum(test_data$has_company_logo == TRUE)) 
  labels <- c("Has Logo", "Doesn't Have Logo") # 
  pie(values, labels = labels, radius = 0.6, donut = TRUE)
```

```{r Has_questions}
# Create a bar plot for Has_questions
ggplot(test_data, aes(x = has_questions)) +
  geom_bar(fill = "orange") +
  labs(x = "Has_questions", y = "Frequency") +
  ggtitle("Frequency of Having Questions")

# Pie chart
  values <- c(sum(test_data$has_questions == FALSE), sum(test_data$has_questions == TRUE)) 
  labels <- c("Has Questions", "Doesn't Have Questions")
  pie(values, labels = labels, radius = 0.6, donut = TRUE)
```

```{r}
write.csv(test_data, "updated_data.csv")
```

We also did some more EDA on some of the other variables and we have included them in a Jupyter Notebook called lstmColumnGeneration in our folder. 
# Mini-Report

Please write a mini-report of your findings so far? What can you say of your data? Do you have any concerns? How well do you think your data is tailored to answering your research question?


We found a lot of null values, so we used various methods of EDA to resolve this issue, mainly by replacing the null values with other values that do not change the trends of the dataset. We won’t have problems with R functions purposely omitting `NA` values. Unfortunately, these “empty” or “unspecified” values still form a huge part of values. For example, in the required_employment and required_education columns, empty values are in the majority of rows. This makes it difficult to perform reliable analysis without omitting large numbers of data.
We think our data is still promising. Through our EDA graphs we can already see some trends, especially in the graphs that consist of fraudulent and non fraudulent emails stacked on top of each other. This leaves us optimistic that we can produce good regressions.

With some more refining, our data and model’s results can 100% answer our research question. We want to focus our efforts on helping disabled people and alleviating their struggles with job searching and obtaining government assistance like medicare and medicaid. By analyzing fraudulent job descriptions, we can decrease the chance that at-risk people fall victim to scams and can use what we learn to focus efforts to redirect these people to vetted programs to provide them assistance and care.

